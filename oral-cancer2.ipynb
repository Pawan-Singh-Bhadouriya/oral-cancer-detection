{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7716037,"sourceType":"datasetVersion","datasetId":4426002},{"sourceId":11202522,"sourceType":"datasetVersion","datasetId":6994461}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torchvision import datasets, transforms, models\nfrom torch.utils.data import DataLoader\nfrom torch.utils.data.dataloader import default_collate\nfrom PIL import ImageFile\nfrom PIL import Image\n\nImageFile.LOAD_TRUNCATED_IMAGES = True\n\ndef safe_loader(path):\n    try:\n        with Image.open(path) as img:\n            return img.convert('RGB')\n    except:\n        return None\n\nclass SafeImageFolder(datasets.ImageFolder):\n    def __getitem__(self, index):\n        path, target = self.samples[index]\n        sample = self.loader(path)\n        if sample is None:\n            # Skip or handle unreadable images\n            return None, None\n        if self.transform is not None:\n            sample = self.transform(sample)\n        return sample, target\n\ndef safe_collate_fn(batch):\n    # Exclude unreadable samples\n    batch = [(x, y) for (x, y) in batch if x is not None and y is not None]\n    if len(batch) == 0:\n        return None, None\n    return default_collate(batch)\n\n# Define device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Define transformations\ntransform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n])\n\n# Load dataset\ndata_dir = \"/kaggle/input/dataset/OC Dataset kaggle new\"\ntrain_data = SafeImageFolder(\n    root=data_dir + '/train',\n    transform=transform,\n    loader=safe_loader\n)\nvalidation_data = SafeImageFolder(\n    root=data_dir + '/valid',\n    transform=transform,\n    loader=safe_loader\n)\n\ntrainloader = DataLoader(train_data, batch_size=32, shuffle=True, collate_fn=safe_collate_fn)\nvalloader = DataLoader(validation_data, batch_size=32, shuffle=False, collate_fn=safe_collate_fn)\n\n# Define model\nmodel = models.resnet18(pretrained=True)\nmodel.fc = nn.Sequential(\n    nn.Linear(model.fc.in_features, 1),\n    nn.Sigmoid()\n)\nmodel = model.to(device)\n\n# Define loss and optimizer\ncriterion = nn.BCEWithLogitsLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\n# Training loop\nepochs = 10\nbest_val_loss = float('inf')\n\nfor epoch in range(epochs):\n    model.train()\n    running_loss = 0.0\n    correct = 0\n    total = 0\n    \n    for inputs, labels in trainloader:\n        inputs, labels = inputs.to(device), labels.to(device).float().unsqueeze(1)\n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item() * inputs.size(0)\n        \n        # Binary prediction threshold at 0.5\n        preds = (outputs >= 0.5).float()\n        correct += (preds == labels).sum().item()\n        total += labels.size(0)\n\n    epoch_loss = running_loss / len(train_data)\n    epoch_acc = correct / total if total > 0 else 0\n    print(f\"Epoch {epoch+1}/{epochs}, Training Loss: {epoch_loss:.4f}, Training Accuracy: {epoch_acc:.4f}\")\n    \n    # Validation loop\n    model.eval()\n    val_loss = 0.0\n    val_correct = 0\n    val_total = 0\n    with torch.no_grad():\n        for inputs, labels in valloader:\n            inputs, labels = inputs.to(device), labels.to(device).float().unsqueeze(1)\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            val_loss += loss.item() * inputs.size(0)\n\n            preds = (outputs >= 0.5).float()\n            val_correct += (preds == labels).sum().item()\n            val_total += labels.size(0)\n\n    val_loss /= len(validation_data)\n    val_acc = val_correct / val_total if val_total > 0 else 0\n    print(f\"Epoch {epoch+1}/{epochs}, Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_acc:.4f}\")\n\n    # Save best model\n    if val_loss < best_val_loss:\n        best_val_loss = val_loss\n        torch.save(model.state_dict(), \"best_model.pth\")\n\nprint(\"Training complete.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T06:40:19.517772Z","iopub.execute_input":"2025-03-29T06:40:19.518071Z","iopub.status.idle":"2025-03-29T06:42:34.633106Z","shell.execute_reply.started":"2025-03-29T06:40:19.518048Z","shell.execute_reply":"2025-03-29T06:42:34.632210Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/PIL/Image.py:1054: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/10, Training Loss: 0.5951, Training Accuracy: 0.8106\nEpoch 1/10, Validation Loss: 0.6582, Validation Accuracy: 0.6633\nEpoch 2/10, Training Loss: 0.5711, Training Accuracy: 0.8606\nEpoch 2/10, Validation Loss: 0.6215, Validation Accuracy: 0.8112\nEpoch 3/10, Training Loss: 0.5463, Training Accuracy: 0.9136\nEpoch 3/10, Validation Loss: 0.7059, Validation Accuracy: 0.6735\nEpoch 4/10, Training Loss: 0.5529, Training Accuracy: 0.8955\nEpoch 4/10, Validation Loss: 0.5765, Validation Accuracy: 0.8520\nEpoch 5/10, Training Loss: 0.5509, Training Accuracy: 0.9000\nEpoch 5/10, Validation Loss: 0.6118, Validation Accuracy: 0.8265\nEpoch 6/10, Training Loss: 0.5385, Training Accuracy: 0.9318\nEpoch 6/10, Validation Loss: 0.6220, Validation Accuracy: 0.6939\nEpoch 7/10, Training Loss: 0.5384, Training Accuracy: 0.9258\nEpoch 7/10, Validation Loss: 0.5905, Validation Accuracy: 0.8316\nEpoch 8/10, Training Loss: 0.5408, Training Accuracy: 0.9258\nEpoch 8/10, Validation Loss: 0.6303, Validation Accuracy: 0.7908\nEpoch 9/10, Training Loss: 0.5443, Training Accuracy: 0.9182\nEpoch 9/10, Validation Loss: 0.5874, Validation Accuracy: 0.8469\nEpoch 10/10, Training Loss: 0.5397, Training Accuracy: 0.9242\nEpoch 10/10, Validation Loss: 0.6691, Validation Accuracy: 0.7245\nTraining complete.\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torchvision import datasets, transforms, models\nfrom torch.utils.data import DataLoader\nfrom torch.utils.data.dataloader import default_collate\nfrom PIL import ImageFile\nfrom PIL import Image\n\nImageFile.LOAD_TRUNCATED_IMAGES = True\n\ndef safe_loader(path):\n    try:\n        with Image.open(path) as img:\n            return img.convert('RGB')\n    except:\n        return None\n\nclass SafeImageFolder(datasets.ImageFolder):\n    def __getitem__(self, index):\n        path, target = self.samples[index]\n        sample = self.loader(path)\n        if sample is None:\n            # Skip or handle unreadable images\n            return None, None\n        if self.transform is not None:\n            sample = self.transform(sample)\n        return sample, target\n\ndef safe_collate_fn(batch):\n    # Exclude unreadable samples\n    batch = [(x, y) for (x, y) in batch if x is not None and y is not None]\n    if len(batch) == 0:\n        return None, None\n    return default_collate(batch)\n\n# Define device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Define transformations\ntransform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n])\n\n# Load dataset\ndata_dir = \"/kaggle/input/dataset/OC Dataset kaggle new\"\ntrain_data = SafeImageFolder(\n    root=data_dir + '/train',\n    transform=transform,\n    loader=safe_loader\n)\nvalidation_data = SafeImageFolder(\n    root=data_dir + '/valid',\n    transform=transform,\n    loader=safe_loader\n)\n\ntrainloader = DataLoader(train_data, batch_size=32, shuffle=True, collate_fn=safe_collate_fn)\nvalloader = DataLoader(validation_data, batch_size=32, shuffle=False, collate_fn=safe_collate_fn)\n\n# Define model with 2 outputs and no sigmoid\nmodel = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\nmodel.fc = nn.Linear(model.fc.in_features, 2)  # 2 classes\nmodel = model.to(device)\n\n# Use CrossEntropyLoss for two-class output\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\nepochs = 10\nbest_val_loss = float('inf')\n\nfor epoch in range(epochs):\n    model.train()\n    running_loss, correct, total = 0.0, 0, 0\n\n    for inputs, labels in trainloader:\n        # Labels: shape [batch_size], values 0 or 1\n        inputs, labels = inputs.to(device), labels.to(device)\n\n        outputs = model(inputs)     # shape [batch_size, 2]\n        loss = criterion(outputs, labels)\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item() * inputs.size(0)\n        _, preds = outputs.max(dim=1)        # get predicted class\n        correct += (preds == labels).sum().item()\n        total += labels.size(0)\n\n    epoch_loss = running_loss / len(train_data)\n    epoch_acc = correct / total if total > 0 else 0\n    print(f\"Epoch {epoch+1}/{epochs}, Training Loss: {epoch_loss:.4f}, Accuracy: {epoch_acc:.4f}\")\n\n    # Validation loop\n    model.eval()\n    val_loss, val_correct, val_total = 0.0, 0, 0\n    with torch.no_grad():\n        for inputs, labels in valloader:\n            inputs, labels = inputs.to(device), labels.to(device)\n            outputs = model(inputs)\n            \n            loss = criterion(outputs, labels)\n            val_loss += loss.item() * inputs.size(0)\n            \n            _, preds = outputs.max(dim=1)\n            val_correct += (preds == labels).sum().item()\n            val_total += labels.size(0)\n\n    val_loss /= len(validation_data)\n    val_acc = val_correct / val_total if val_total > 0 else 0\n    print(f\"Epoch {epoch+1}/{epochs}, Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_acc:.4f}\")\n\n    if val_loss < best_val_loss:\n        best_val_loss = val_loss\n        torch.save(model.state_dict(), \"best_model2.pth\")\n        \nprint(\"Training complete.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T06:43:41.954452Z","iopub.execute_input":"2025-03-29T06:43:41.954701Z","iopub.status.idle":"2025-03-29T06:46:08.078309Z","shell.execute_reply.started":"2025-03-29T06:43:41.954670Z","shell.execute_reply":"2025-03-29T06:46:08.077164Z"}},"outputs":[{"name":"stderr","text":"Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n100%|██████████| 44.7M/44.7M [00:00<00:00, 174MB/s] \n/usr/local/lib/python3.10/dist-packages/PIL/Image.py:1054: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/10, Training Loss: 0.5018, Accuracy: 0.8091\nEpoch 1/10, Validation Loss: 9.3561, Validation Accuracy: 0.5663\nEpoch 2/10, Training Loss: 0.3397, Accuracy: 0.8576\nEpoch 2/10, Validation Loss: 0.9228, Validation Accuracy: 0.6480\nEpoch 3/10, Training Loss: 0.1394, Accuracy: 0.9500\nEpoch 3/10, Validation Loss: 0.6016, Validation Accuracy: 0.7806\nEpoch 4/10, Training Loss: 0.0492, Accuracy: 0.9848\nEpoch 4/10, Validation Loss: 0.4055, Validation Accuracy: 0.8469\nEpoch 5/10, Training Loss: 0.1001, Accuracy: 0.9636\nEpoch 5/10, Validation Loss: 0.9772, Validation Accuracy: 0.8163\nEpoch 6/10, Training Loss: 0.1393, Accuracy: 0.9485\nEpoch 6/10, Validation Loss: 0.6925, Validation Accuracy: 0.8520\nEpoch 7/10, Training Loss: 0.0951, Accuracy: 0.9758\nEpoch 7/10, Validation Loss: 0.4507, Validation Accuracy: 0.8571\nEpoch 8/10, Training Loss: 0.0465, Accuracy: 0.9833\nEpoch 8/10, Validation Loss: 1.1749, Validation Accuracy: 0.7806\nEpoch 9/10, Training Loss: 0.0353, Accuracy: 0.9894\nEpoch 9/10, Validation Loss: 0.5145, Validation Accuracy: 0.8622\nEpoch 10/10, Training Loss: 0.0223, Accuracy: 0.9955\nEpoch 10/10, Validation Loss: 0.5500, Validation Accuracy: 0.8316\nTraining complete.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}